{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def enhance_speech(audio_file_path, output_file_path):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Perform noise reduction\n",
    "    reduced_noise = nr.reduce_noise(y=y, sr=sr)\n",
    "    \n",
    "    # Save the enhanced audio to a file\n",
    "    # librosa.output.write_wav(output_file_path, reduced_noise, sr)\n",
    "    sf.write(output_file_path, reduced_noise, sr)\n",
    "\n",
    "# Usage example\n",
    "enhance_speech('../data/recorded_audio.wav', '../data/recorded_audio_nr.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No speech signal detected.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "import io\n",
    "\n",
    "def is_speech_present(audio_file_path, silence_thresh=-50, min_silence_len=500):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        \n",
    "        # Detect non-silent chunks in the audio file\n",
    "        non_silent_chunks = detect_nonsilent(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
    "        \n",
    "        # Check if there are any non-silent chunks\n",
    "        if len(non_silent_chunks) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Use speech recognition to confirm if there is speech in the non-silent chunks\n",
    "        recognizer = sr.Recognizer()\n",
    "        for chunk in non_silent_chunks:\n",
    "            start, end = chunk\n",
    "            audio_chunk = audio[start:end]\n",
    "            \n",
    "            # Export audio chunk to memory buffer\n",
    "            audio_chunk_buffer = io.BytesIO()\n",
    "            audio_chunk.export(audio_chunk_buffer, format=\"wav\")\n",
    "            audio_chunk_buffer.seek(0)\n",
    "            \n",
    "            with sr.AudioFile(audio_chunk_buffer) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "                try:\n",
    "                    recognizer.recognize_google(audio_data)\n",
    "                    return True\n",
    "                except sr.UnknownValueError:\n",
    "                    continue\n",
    "                except sr.RequestError as e:\n",
    "                    print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in checking speech presence: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"../data/noise.wav\"\n",
    "speech_present = is_speech_present(audio_file)\n",
    "if speech_present:\n",
    "    print(\"Speech signal detected.\")\n",
    "else:\n",
    "    print(\"No speech signal detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /home/keagan/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/keagan/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-06-28 16:10:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from df import enhance, init_df\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "\n",
    "def add_gain(waveform, gain_db):\n",
    "    gain = 10 ** (gain_db / 20)\n",
    "    return waveform * gain\n",
    "\n",
    "def apply_filters(waveform, sample_rate):\n",
    "    # Apply high-pass filter\n",
    "    highpass_waveform = F.highpass_biquad(waveform, sample_rate, cutoff_freq=80)\n",
    "    # Apply low-pass filter\n",
    "    lowpass_waveform = F.lowpass_biquad(highpass_waveform, sample_rate, cutoff_freq=8000)\n",
    "    return lowpass_waveform\n",
    "\n",
    "mode_df, df_state, _ = init_df()  # Load default model\n",
    "\n",
    "gain_db = 10\n",
    "in_file_path=\"../data/kaegan.wav\"\n",
    "# y, sr = librosa.load(\"../data/recorded_audio.wav\", sr=None)\n",
    "waveform, sample_rate = torchaudio.load(in_file_path)\n",
    "# highpass = T.Highpass(sample_rate=sample_rate, cutoff_freq=80)\n",
    "# lowpass = T.Lowpass(sample_rate=sample_rate, cutoff_freq=8000)\n",
    "enhanced_audio = enhance(mode_df, df_state, waveform)\n",
    "filtered_waveform = apply_filters(enhanced_audio, sample_rate)\n",
    "# enhanced_waveform_with_gain = add_gain(enhanced_audio, gain_db)\n",
    "torchaudio.save('../data/kaegan_df.wav', filtered_waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(\"../data/kaegan.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello this is a test for audio customer acquisition on Myntra So my name is Keegan William Denise Can you please use this message as a test\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Hello this is a test for audio customer acquisition on Myntra So my name is Keegan William Denise Can you please use this message as a test',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 7.0,\n",
       "   'text': ' Hello this is a test for audio customer acquisition on Myntra',\n",
       "   'tokens': [50365,\n",
       "    2425,\n",
       "    341,\n",
       "    307,\n",
       "    257,\n",
       "    1500,\n",
       "    337,\n",
       "    6278,\n",
       "    5474,\n",
       "    21668,\n",
       "    322,\n",
       "    1222,\n",
       "    580,\n",
       "    424,\n",
       "    50715],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.3989851152574694,\n",
       "   'compression_ratio': 1.2636363636363637,\n",
       "   'no_speech_prob': 0.012981951236724854},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 7.0,\n",
       "   'end': 10.0,\n",
       "   'text': ' So my name is Keegan William Denise',\n",
       "   'tokens': [50715, 407, 452, 1315, 307, 3189, 43118, 6740, 38133, 50865],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.3989851152574694,\n",
       "   'compression_ratio': 1.2636363636363637,\n",
       "   'no_speech_prob': 0.012981951236724854},\n",
       "  {'id': 2,\n",
       "   'seek': 0,\n",
       "   'start': 10.0,\n",
       "   'end': 15.36,\n",
       "   'text': ' Can you please use this message as a test',\n",
       "   'tokens': [50865, 1664, 291, 1767, 764, 341, 3636, 382, 257, 1500, 51133],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.3989851152574694,\n",
       "   'compression_ratio': 1.2636363636363637,\n",
       "   'no_speech_prob': 0.012981951236724854}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Hello, this is Kigal, we need to use the text for external acquisition for the trap. Please use this on the equipment.\n",
    "# Hello this is Ken from the RG 2014 M<|ml|> Symbols Of Trap\n",
    "#  Hello, this is Kenan. We're going to use the text for customer information for Kinect. Please use this for the peer-to-peer chat.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
